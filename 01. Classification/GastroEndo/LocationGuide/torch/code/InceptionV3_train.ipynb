{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import copy\n",
    "import yaml\n",
    "import pickle\n",
    "from PIL import Image\n",
    "\n",
    "from util.dataload import DataIOStream, CustomDataset\n",
    "from util.create_data import DataCreateStream\n",
    "# from model.model import CreateGastricModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path = os.path.join(\"..\", \"config\", \"train.yaml\")\n",
    "# load config.yaml\n",
    "with open(yaml_path) as f:\n",
    "    conf = yaml.safe_load(f)\n",
    "print('=='*50)\n",
    "for item in conf:\n",
    "    print(f'{item}: {conf[item]}')\n",
    "print('=='*50)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]='PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(conf['gpu'])\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "start_time = time.time()\n",
    "d = datetime.datetime.now()\n",
    "now_time = f\"{d.year}/{d.month}/{d.day} {d.hour}:{d.minute}:{d.second}\"\n",
    "print(f\"Now time: {now_time}\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imgShape(model_name):\n",
    "    dict_modelIMGShape = {\"AlexNet\":224, \"ConvNeXt\":224, \"DenseNet\":224, \"EfficientNet\":224, \"EfficientNetV2\":224,\n",
    "                          \"GoogLeNet\":224, \"InceptionV3\":512, \"MaxVit\":224, \"MNASNet\":224, \"MobileNetV2\":224,\n",
    "                          \"MobileNet\":224, \"RegNet\":224, \"ResNet\":224, \"ResNeXt\":224, \"ShuffleNetV2\":224,\n",
    "                          \"SqueezeNet\":224, \"SwinTransformer\":224, \"VGG\":224, \"VisionTransformer\":224, \"WideResNet\":224,\n",
    "    }\n",
    "    if model_name not in list(dict_modelIMGShape.keys()):\n",
    "        imgShape = 512\n",
    "    else:\n",
    "        imgShape = dict_modelIMGShape[model_name]\n",
    "    \n",
    "    return imgShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_keys_box = list(conf[\"class_box\"].keys()) # ex) [\"ES\", \"GE\", \"xx\", \"xx\"]\n",
    "class_values_box = list(conf[\"class_box\"].values()) # ex) [[\"ES\", \"GE\"], [\"UB\", \"MB\", \"LB\"], [\"xx\", \"xx\"]]\n",
    "img_size = load_imgShape(model_name=conf[\"model_name\"])\n",
    "# npy 디텍토리 내부, 클래스 범주를 기준으로 폴더를 구성한다.\n",
    "class_fpath = os.path.join(\"..\", \"..\", \"npy\", \"_\".join(class_keys_box))\n",
    "if not os.path.exists(class_fpath):\n",
    "    os.mkdir(class_fpath)\n",
    "# npy/{class_fpath} 디렉토리 내부, 이미지 사이즈를 기준으로 폴더를 구성한다.\n",
    "size_fpath = os.path.join(\"..\", \"..\", \"npy\", \"_\".join(class_keys_box), str(img_size))\n",
    "if not os.path.exists(size_fpath):\n",
    "    os.mkdir(size_fpath)\n",
    "    data_dir = os.path.join(\"..\", \"..\", \"data\", \"class_box\")\n",
    "    train_dict, test_dict, val_dict = DataCreateStream().data_split(class_box=class_values_box,\n",
    "                                                                    data_dir=data_dir,\n",
    "    )\n",
    "    with open(file=os.path.join(size_fpath, \"train.pickle\"), mode='wb') as f:\n",
    "        pickle.dump(train_dict, f, protocol=4)\n",
    "    with open(file=os.path.join(size_fpath, \"test.pickle\"), mode='wb') as f:\n",
    "        pickle.dump(test_dict, f, protocol=4)\n",
    "    with open(file=os.path.join(size_fpath, \"val.pickle\"), mode='wb') as f:\n",
    "        pickle.dump(val_dict, f, protocol=4)\n",
    "# load dataset\n",
    "train_dict, test_dict, val_dict = DataIOStream().dataloader_all(path=size_fpath)\n",
    "train_set = CustomDataset(x_data_paths=train_dict[\"input_path\"],\n",
    "                          y_data=train_dict[\"input_label\"],\n",
    "                          img_size=img_size,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                           batch_size=conf[\"batch\"],\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=0,\n",
    ")\n",
    "test_set = CustomDataset(x_data_paths=test_dict[\"input_path\"],\n",
    "                         y_data=test_dict[\"input_label\"],\n",
    "                         img_size=img_size,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                          batch_size=conf[\"batch\"],\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=0,\n",
    ")\n",
    "val_set = CustomDataset(x_data_paths=val_dict[\"input_path\"],\n",
    "                        y_data=val_dict[\"input_label\"],\n",
    "                        img_size=img_size,\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(val_set,\n",
    "                                         batch_size=conf[\"batch\"],\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=0,\n",
    ")\n",
    "data_loaders = {\"train\": train_loader,\n",
    "                \"test\": test_loader,\n",
    "                \"val\": val_loader,\n",
    "}\n",
    "dataset_sizes = {\"train\": len(train_dict[\"input_path\"]),\n",
    "                 \"test\": len(test_dict[\"input_path\"]),\n",
    "                 \"val\": len(val_dict[\"input_path\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지를 보여주기 위한 함수\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "# 학습용 이미지를 무작위로 가져오기\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "# 이미지 보여주기\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "y_ts = []\n",
    "for label in labels:\n",
    "    y_ts.append(class_keys_box[np.argmax(label.numpy(), axis=0)])\n",
    "print(y_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                criterion, \n",
    "                optimizer, \n",
    "                scheduler, \n",
    "                num_epochs=conf[\"epoch\"]\n",
    "):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}/{num_epochs-1}\")\n",
    "        print('-'*10)\n",
    "        # 각 에폭 (epoch)은 학습 단계와 검증 단계를 갖습니다.\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()    # 모델을 학습 모드로 설정합니다.\n",
    "            else:\n",
    "                model.eval()     # 모델을 평가 모드로 설정합니다.\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            # 데이터를 반복합니다.\n",
    "            for inputs, labels in data_loaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # 매개변수 경사도를 0으로 설정합니다.\n",
    "                optimizer.zero_grad()\n",
    "                # 순전파\n",
    "                # 학습시에만 연산 기록을 추적합니다.\n",
    "                with torch.set_grad_enabled(phase==\"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    try:\n",
    "                        _, preds = torch.max(outputs.logits.to(torch.float64), -1)\n",
    "                        loss = criterion(outputs.logits.to(torch.float64), labels)\n",
    "                    except:\n",
    "                        _, preds = torch.max(outputs.to(torch.float64), -1)\n",
    "                        loss = criterion(outputs.to(torch.float64), labels)\n",
    "                    # 학습 단계인 경우 역전파 + 최적화\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                # 통계\n",
    "                _, trues = torch.max(labels.data, -1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == trues)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            # 모델을 깊은 복사(deep copy)함\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # 가장 나은 모델 가중치를 불러옴\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, \n",
    "                    num_images=6\n",
    "):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(data_loaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, -1)\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'predicted: {class_keys_box[preds[j]]}')\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정 필요\n",
    "# https://pytorch.org/vision/0.14/models.html#classification\n",
    "weights = models.Inception_V3_Weights.IMAGENET1K_V1\n",
    "model_ft = models.inception_v3(weights=weights)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# ``nn.Linear(num_ftrs, len (class_names))`` 로 일반화할 수 있습니다.\n",
    "model_ft.fc = nn.Linear(num_ftrs, len(class_keys_box))\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 모든 매개변수들이 최적화되었는지 관찰\n",
    "if conf[\"optimizer\"] == \"SGD\":\n",
    "    optimizer_ft = optim.SGD(model_ft.parameters(), \n",
    "                             lr=conf[\"initial_learning_rate\"], \n",
    "                             momentum=0.9\n",
    "    )\n",
    "# N 에폭마다 0.1씩 학습률 감소\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, \n",
    "                                       step_size=7, \n",
    "                                       gamma=conf[\"learning_rate_gamma\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, \n",
    "                       criterion, \n",
    "                       optimizer_ft, \n",
    "                       exp_lr_scheduler,\n",
    "                       num_epochs=conf[\"epoch\"],\n",
    ")\n",
    "# mkdir\n",
    "number = \"0000\"\n",
    "fname = f\"{conf['model_name']}_{conf['model_weight']}_{number}\"\n",
    "exp_path0 = os.path.join(\"..\", \"exp\", \"_\".join(class_keys_box))\n",
    "if not os.path.exists(exp_path0):\n",
    "    os.mkdir(exp_path0)\n",
    "exp_path1 = os.path.join(exp_path0, fname)\n",
    "while os.path.exists(exp_path1):\n",
    "    number = f\"{int(number)+1:04d}\"\n",
    "    fname = f\"{conf['model_name']}_{conf['model_weight']}_{number}\"\n",
    "    exp_path1 = os.path.join(\"..\", \"exp\", \"_\".join(class_keys_box), fname)\n",
    "os.mkdir(exp_path1)\n",
    "# save model\n",
    "torch.save(model_ft, \n",
    "           os.path.join(exp_path1, \"model.pth\")\n",
    ")\n",
    "# save model with onnx\n",
    "# Input to the model\n",
    "x = torch.randn(conf[\"batch\"], 1, 224, 224, requires_grad=True)\n",
    "torch_out = model_ft(x)\n",
    "\n",
    "onnx_path = os.path.join(\"..\", \"exp\", \"ES-GE\", \"inception_v3_IMAGENET1K_V1_0000\", \"model.onnx\")\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model_ft,               # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  onnx_path,                 # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"..\", \"exp\", \"ES-GE\", \"inception_v3_IMAGENET1K_V1_0000\", \"model.pth\")\n",
    "model = torch.load(model_path)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(model, \n",
    "                  input: torch.tensor,\n",
    "):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input = input.to(device)\n",
    "        output = model(input)\n",
    "        _, pred = torch.max(output, -1)\n",
    "        model.train(mode=was_training)\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ksm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
